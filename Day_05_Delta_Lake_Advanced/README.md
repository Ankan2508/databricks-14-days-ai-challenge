# Day 05 â€“ Delta Lake Advanced

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on strengthening advanced data engineering concepts using Delta Lake on Databricks.

## ðŸŽ¯ Objective
The objective of Day 05 was to explore **advanced Delta Lake features** that make data lakes production-ready, including versioning, incremental updates, performance optimization, and storage cleanup.

## ðŸ“˜ Topics Covered
- Delta Lake time travel and version history
- Querying historical data using versions and timestamps
- MERGE operations for incremental upserts
- Handling inserts, updates, and deletes using MERGE
- Table optimization using `OPTIMIZE`
- Data skipping and performance tuning with `ZORDER`
- File cleanup using `VACUUM`

## ðŸ§ª Hands-on Tasks
- Implemented incremental data updates using `MERGE INTO`
- Queried previous versions of Delta tables using time travel
- Optimized Delta tables to improve query performance
- Cleaned up obsolete data files using `VACUUM`

## ðŸ§  Key Learnings
Through this exercise, I learned:
- How Delta Lake enables reliable **incremental data pipelines**
- The practical value of time travel for debugging and auditing
- How `OPTIMIZE` and `ZORDER` improve query performance
- Why `VACUUM` is essential for managing storage and cost
- How Delta Lake simplifies complex data engineering workflows

## ðŸ›  Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Delta Lake
- Spark SQL

ðŸ“Œ This notebook represents **Day 05 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
