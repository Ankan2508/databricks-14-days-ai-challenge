# Day 07 â€“ Workflows & Job Orchestration in Databricks

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on understanding how to operationalize data pipelines using Databricks workflows and job orchestration.

## ðŸŽ¯ Objective
The objective of Day 07 was to learn how to move from individual notebooks to **automated, production-ready workflows** using Databricks Jobs, multi-task pipelines, and scheduling.

## ðŸ“˜ Topics Covered
- Difference between Databricks notebooks and Databricks Jobs
- Designing multi-task workflows
- Parameterizing notebooks using widgets
- Passing parameters between tasks
- Setting task dependencies (Bronze â†’ Silver â†’ Gold)
- Scheduling workflows
- Error handling and retry strategies

## ðŸ§ª Hands-on Tasks
- Added parameter widgets to notebooks
- Created a multi-task Databricks Job
- Orchestrated Bronze â†’ Silver â†’ Gold tasks
- Configured task dependencies
- Scheduled workflow execution
- Observed error handling behavior in workflows

## ðŸ§  Key Learnings
Through this exercise, I learned:
- How Databricks Jobs enable pipeline automation
- The importance of task dependencies in workflow orchestration
- How parameterization makes pipelines reusable
- How scheduling and retries support reliable data operations
- How orchestration connects engineering logic with business SLAs

## ðŸ›  Tools & Technologies
- Databricks Community Edition
- Databricks Workflows (Jobs)
- Apache Spark (PySpark)
- Delta Lake

ðŸ“Œ This notebook represents **Day 07 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
