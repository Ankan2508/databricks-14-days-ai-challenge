# Day 07 â€“ Databricks Jobs & Workflows

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on building production-ready data pipelines using Databricks Jobs and workflow orchestration.

## ðŸŽ¯ Objective
The objective of Day 07 was to understand how **Databricks Jobs differ from notebooks**, and to design and schedule **multi-task workflows** with parameters, dependencies, and error handling.

## ðŸ“˜ Topics Covered
- Databricks Jobs vs interactive notebooks
- Designing multi-task workflows
- Passing parameters using Databricks widgets
- Setting task dependencies (Bronze â†’ Silver â†’ Gold)
- Scheduling automated job execution
- Basic error handling and failure management

## ðŸ§ª Hands-on Tasks
- Added parameter widgets to notebooks
- Created a multi-task Databricks Job
- Orchestrated Bronze â†’ Silver â†’ Gold tasks
- Defined task dependencies between layers
- Scheduled workflow execution
- Observed error behavior and retry mechanisms

## ðŸ§  Key Learnings
Through this exercise, I learned:
- How Databricks Jobs enable automated, repeatable pipelines
- Why separating development notebooks from production jobs matters
- How parameters make workflows reusable and dynamic
- The importance of dependencies and error handling in orchestration
- How Databricks simplifies workflow scheduling

## ðŸ›  Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Delta Lake
- Databricks Jobs
- Spark SQL

ðŸ“Œ This notebook represents **Day 07 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
