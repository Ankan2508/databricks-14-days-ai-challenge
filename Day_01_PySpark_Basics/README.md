# Day 01 â€“ PySpark Basics in Databricks

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on building strong foundations in Databricks and Apache Spark through hands-on practice.

## ðŸŽ¯ Objective
The objective of Day 1 was to understand the Databricks workspace, set up compute, and work with PySpark DataFrames while bridging PySpark and SQL workflows.

## ðŸ“˜ Topics Covered
- SparkSession and its role in Databricks
- Creating PySpark DataFrames from in-memory data
- Understanding schema using `printSchema()`
- Data exploration using `show()`, `count()`, `select()`
- Filtering using conditional expressions
- Adding derived columns
- Aggregations using `groupBy()`
- Creating temporary views
- Running Spark SQL queries

## ðŸ§  Key Learnings
Through this notebook, I learned:
- How PySpark enables distributed data processing
- Seamless switching between PySpark and SQL
- The Lakehouse-style workflow for analytics and engineering

## ðŸ›  Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Spark SQL

ðŸ“Œ This notebook represents **Day 1 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
