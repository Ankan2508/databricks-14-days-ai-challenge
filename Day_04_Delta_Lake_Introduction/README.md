# Day 04 â€“ Delta Lake Introduction

This notebook is part of the **Databricks 14-Day AI Challenge**, focused on building strong foundations in modern data engineering using Databricks and Apache Spark.

## ðŸŽ¯ Objective
The objective of Day 04 was to understand **Delta Lake fundamentals** and learn how Delta improves reliability, consistency, and data management over traditional file formats like Parquet.

## ðŸ“˜ Topics Covered
- What is Delta Lake and why it is needed
- ACID transactions in big data systems
- Schema enforcement in Delta tables
- Differences between Delta Lake and Parquet
- Converting CSV data to Delta format
- Creating Delta tables using:
  - PySpark
  - Spark SQL
- Handling duplicate inserts
- Testing schema enforcement behavior

## ðŸ§ª Hands-on Tasks
- Converted CSV data into Delta format
- Created managed Delta tables using PySpark
- Created Delta tables using Spark SQL
- Tested schema enforcement by inserting mismatched schemas
- Observed error handling during invalid writes

## ðŸ§  Key Learnings
Through this exercise, I learned:
- How Delta Lake brings **ACID guarantees** to data lakes
- Why schema enforcement is critical for data quality
- How Delta tables prevent accidental corruption of data
- The practical advantages of Delta Lake over Parquet for production pipelines

## ðŸ›  Tools & Technologies
- Databricks Community Edition
- Apache Spark (PySpark)
- Delta Lake
- Spark SQL

ðŸ“Œ This notebook represents **Day 04 progress** in my Databricks learning journey, guided by **Indian Data Club**, **Codebasics**, and **Databricks**.
